from typing import Literal

from agent.tools.base import BaseTool, ToolSchema, ToolParameter
from core.crawler import fetch_all_contents
from core.crawler.search_engine import get_search_client, search
from core.models.search import SearchResult


class FetchWebContentsTool(BaseTool[dict[str, str]]):
    """æ‰¹é‡è·å–ç½‘é¡µå†…å®¹çš„å·¥å…·"""

    @property
    def name(self) -> str:
        return "fetch_web_contents"

    @property
    def schema(self) -> ToolSchema:
        return ToolSchema(
            name=self.name,
            description=(
                "æ‰¹é‡æŠ“å–æŒ‡å®š URL åˆ—è¡¨çš„ç½‘é¡µæ­£æ–‡å†…å®¹ã€‚ä½¿ç”¨å¼‚æ­¥å¹¶å‘è¯·æ±‚æé«˜æ•ˆç‡ï¼Œ"
                "è‡ªåŠ¨å¤„ç†å„ç§ç½‘é¡µæ ¼å¼ï¼Œæå–ä¸»è¦æ–‡æœ¬å†…å®¹å¹¶æ¸…ç† HTML æ ‡ç­¾ã€‚"
                "é€‚ç”¨äºéœ€è¦è·å–ç½‘é¡µè¯¦ç»†å†…å®¹è¿›è¡Œåˆ†ææˆ–æ‘˜è¦çš„åœºæ™¯ã€‚"
            ),
            parameters=[
                ToolParameter(
                    name="urls",
                    type="list[str]",
                    description="éœ€è¦æŠ“å–å†…å®¹çš„ URL åˆ—è¡¨ã€‚æ”¯æŒ HTTP å’Œ HTTPS åè®®",
                    required=True,
                ),
            ],
            returns=(
                "è¿”å› dict[str, str]ï¼Œé”®ä¸º URLï¼Œå€¼ä¸ºæŠ“å–åˆ°çš„ç½‘é¡µæ­£æ–‡å†…å®¹ã€‚\n"
                "å¦‚æœæŸä¸ª URL æŠ“å–å¤±è´¥ï¼Œè¯¥ URL å¯¹åº”çš„å€¼ä¸ºç©ºå­—ç¬¦ä¸²"
            ),
            when_to_use=(
                "åœ¨ä»¥ä¸‹åœºæ™¯ä½¿ç”¨æ­¤å·¥å…·ï¼š\n"
                "1. æœç´¢å¼•æ“è¿”å›ç»“æœåï¼Œéœ€è¦è·å–å®Œæ•´ç½‘é¡µå†…å®¹\n"
                "2. éœ€è¦è¡¥å…… RSS æ‘˜è¦ä¿¡æ¯ï¼Œè·å–åŸæ–‡å…¨æ–‡\n"
                "3. éªŒè¯æˆ–æ·±å…¥åˆ†ææŸä¸ªæ–°é—»æºçš„è¯¦ç»†å†…å®¹"
            ),
            usage_examples=[
                "fetch_web_contents(['https://example.com/news1', 'https://example.com/news2'])",
                "fetch_web_contents(urls) - ä¼ å…¥ä»æœç´¢ç»“æœä¸­æå–çš„ URL åˆ—è¡¨",
            ],
            notes=[
                "æŠ“å–è¿‡ç¨‹æ˜¯å¼‚æ­¥å¹¶å‘çš„ï¼Œå¤§é‡ URL æ—¶æ•ˆç‡è¾ƒé«˜",
                "æŸäº›ç½‘ç«™å¯èƒ½å› åçˆ¬æœºåˆ¶å¯¼è‡´æŠ“å–å¤±è´¥",
                "è¿”å›çš„å†…å®¹å·²ç»è¿‡æ¸…ç†ï¼Œä¸åŒ…å« HTML æ ‡ç­¾",
                "å»ºè®®æ§åˆ¶å•æ¬¡è¯·æ±‚çš„ URL æ•°é‡ï¼Œé¿å…è¢«ç›®æ ‡ç½‘ç«™å°ç¦",
            ],
        )

    async def _execute(self, urls: list[str]) -> dict[str, str]:
        """
        è·å–ç½‘é¡µå†…å®¹

        Args:
            urls: URLåˆ—è¡¨

        Returns:
            URLåˆ°å†…å®¹çš„æ˜ å°„
        """
        if not urls:
            return {}

        return await fetch_all_contents(urls)


class WebSearchTool(BaseTool[list[SearchResult]]):
    """ç½‘é¡µæœç´¢å·¥å…·"""

    @property
    def name(self) -> str:
        return "search_web"

    @property
    def schema(self) -> ToolSchema:
        return ToolSchema(
            name=self.name,
            description=(
                "ä½¿ç”¨æœç´¢å¼•æ“æœç´¢äº’è”ç½‘å†…å®¹ï¼Œå¹¶è‡ªåŠ¨æŠ“å–æœç´¢ç»“æœé¡µé¢çš„æ­£æ–‡ã€‚"
                "è¯¥å·¥å…·æ•´åˆäº†æœç´¢å’Œå†…å®¹æŠ“å–ä¸¤ä¸ªæ­¥éª¤ï¼š\n"
                "1. è°ƒç”¨æœç´¢å¼•æ“ API è·å–æœç´¢ç»“æœ\n"
                "2. å¹¶å‘æŠ“å–æ‰€æœ‰ç»“æœé¡µé¢çš„æ­£æ–‡å†…å®¹\n"
                "3. è¿‡æ»¤æ‰æŠ“å–å¤±è´¥çš„ç»“æœï¼Œåªè¿”å›æœ‰æ•ˆå†…å®¹\n"
                "é€‚ç”¨äºéœ€è¦è·å–æœ€æ–°äº’è”ç½‘ä¿¡æ¯ã€è¡¥å……èƒŒæ™¯çŸ¥è¯†çš„åœºæ™¯ã€‚"
            ),
            parameters=[
                ToolParameter(
                    name="query",
                    type="str",
                    description=(
                        "æœç´¢æŸ¥è¯¢è¯­å¥ã€‚æ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œä¹Ÿæ”¯æŒæœç´¢å¼•æ“é«˜çº§è¯­æ³•ã€‚"
                        "å»ºè®®ä½¿ç”¨å…·ä½“ã€æ˜ç¡®çš„å…³é”®è¯ç»„åˆä»¥è·å¾—æ›´ç²¾å‡†çš„ç»“æœ"
                    ),
                    required=True,
                ),
                ToolParameter(
                    name="time_range",
                    type="Literal['day', 'week', 'month', 'year']",
                    description=(
                        "æœç´¢ç»“æœçš„æ—¶é—´èŒƒå›´é™åˆ¶ï¼š\n"
                        "  - 'day': è¿‡å»24å°æ—¶\n"
                        "  - 'week': è¿‡å»ä¸€å‘¨\n"
                        "  - 'month': è¿‡å»ä¸€ä¸ªæœˆ\n"
                        "  - 'year': è¿‡å»ä¸€å¹´"
                    ),
                    required=False,
                    default="week",
                ),
                ToolParameter(
                    name="max_results",
                    type="int",
                    description="æœŸæœ›è¿”å›çš„æœ€å¤§ç»“æœæ•°é‡ã€‚å®é™…è¿”å›æ•°é‡å¯èƒ½å› æŠ“å–å¤±è´¥è€Œå‡å°‘",
                    required=False,
                    default="5",
                ),
            ],
            returns=(
                "è¿”å› list[SearchResult]ï¼Œæ¯ä¸ªç»“æœåŒ…å«ï¼š\n"
                "  - title: ç½‘é¡µæ ‡é¢˜\n"
                "  - url: ç½‘é¡µ URL\n"
                "  - content: ç½‘é¡µæ­£æ–‡å†…å®¹ï¼ˆå·²æŠ“å–ï¼‰\n"
                "  - score: æœç´¢ç›¸å…³æ€§è¯„åˆ†"
            ),
            when_to_use=(
                "åœ¨ä»¥ä¸‹åœºæ™¯ä½¿ç”¨æ­¤å·¥å…·ï¼š\n"
                "1. éœ€è¦è¡¥å……å½“å‰æ–°é—»çš„èƒŒæ™¯ä¿¡æ¯æˆ–ä¸Šä¸‹æ–‡\n"
                "2. éªŒè¯æŸä¸ªäº‹ä»¶æˆ–å£°æ˜çš„çœŸå®æ€§\n"
                "3. è·å–æŸä¸ªè¯é¢˜çš„æœ€æ–°å‘å±•åŠ¨æ€\n"
                "4. RSS æºæœªè¦†ç›–çš„ä¿¡æ¯éœ€è¦ä»äº’è”ç½‘è¡¥å……"
            ),
            usage_examples=[
                "search_web('OpenAI GPT-5 å‘å¸ƒ', time_range='day') - æœç´¢æœ€è¿‘ä¸€å¤©çš„ GPT-5 ç›¸å…³æ–°é—»",
                "search_web('ç‰¹æ–¯æ‹‰è´¢æŠ¥', time_range='week', max_results=10) - æœç´¢ä¸€å‘¨å†…ç‰¹æ–¯æ‹‰è´¢æŠ¥ç›¸å…³çš„10æ¡ç»“æœ",
                "search_web('äººå·¥æ™ºèƒ½ç›‘ç®¡æ”¿ç­–', time_range='month') - æœç´¢æœ€è¿‘ä¸€ä¸ªæœˆçš„ AI ç›‘ç®¡æ”¿ç­–æ–°é—»",
            ],
            notes=[
                "éƒ¨åˆ†æœç´¢ç»“æœå¯èƒ½å› ç½‘é¡µæŠ“å–å¤±è´¥è€Œè¢«è¿‡æ»¤",
                "æœç´¢å¼•æ“æœ‰è¯·æ±‚é¢‘ç‡é™åˆ¶ï¼Œè¯·å‹¿é¢‘ç¹è°ƒç”¨",
                "è¿”å›ç»“æœå·²æŒ‰ç›¸å…³æ€§æ’åº",
            ],
        )

    async def _execute(
        self,
        query: str,
        time_range: Literal["day", "week", "month", "year"] = "week",
        max_results: int = 5,
    ) -> list[SearchResult]:
        """
        æœç´¢ç½‘é¡µ

        Args:
            query: æœç´¢æŸ¥è¯¢
            time_range: æ—¶é—´èŒƒå›´
            max_results: æœ€å¤§ç»“æœæ•°

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not query or not query.strip():
            raise ValueError("æœç´¢æŸ¥è¯¢ä¸èƒ½ä¸ºç©º")

        if max_results <= 0:
            raise ValueError("max_results å¿…é¡»å¤§äº 0")

        # æ£€æŸ¥æœç´¢å¼•æ“æ˜¯å¦å¯ç”¨
        if not get_search_client():
            raise RuntimeError("æœç´¢å¼•æ“æœªé…ç½®æˆ–ä¸å¯ç”¨ï¼Œè¯·å…ˆæ£€æŸ¥é…ç½®")

        search_results = search(
            query,
            time_range=time_range,
            max_results=max_results,
        )

        if not search_results:
            return []

        url_map = {result["url"]: result for result in search_results}
        contents = await fetch_all_contents(list(url_map.keys()))

        # ç»Ÿè®¡æŠ“å–ç»“æœ
        total = len(search_results)
        success = sum(1 for r in search_results if contents.get(r["url"]))
        failed = total - success
        if failed > 0:
            print(f"[SEARCH] ğŸ“Š æŠ“å–ç»Ÿè®¡: æˆåŠŸ {success}/{total}, å¤±è´¥ {failed} æ¡")

        # è¿‡æ»¤æ‰è·å–å†…å®¹å¤±è´¥çš„ç»“æœ
        return [
            SearchResult(
                title=result["title"],
                url=result["url"],
                content=contents.get(result["url"], ""),
                score=result["score"],
            )
            for result in search_results
            if contents.get(result["url"])
        ]


# åˆ›å»ºå·¥å…·å®ä¾‹
fetch_web_contents_tool = FetchWebContentsTool()
web_search_tool = WebSearchTool()


# ä¿ç•™åŸæœ‰å‡½æ•°æ¥å£ä»¥å…¼å®¹ç°æœ‰ä»£ç 
async def fetch_web_contents(urls: list[str]) -> dict[str, str]:
    """è·å–ç½‘é¡µå†…å®¹ï¼ˆå…¼å®¹å‡½æ•°ï¼‰

    æ³¨æ„ï¼šæ­¤å‡½æ•°ä¸ºå…¼å®¹æ¥å£ï¼Œç›´æ¥è¿”å›æ•°æ®è€Œé ToolResultã€‚
    æ–°ä»£ç å»ºè®®ä½¿ç”¨ fetch_web_contents_tool.execute() è·å–å¸¦é”™è¯¯å¤„ç†çš„ç»“æœã€‚
    """
    result = await fetch_web_contents_tool.execute(urls)
    if result.success:
        return result.data
    raise RuntimeError(result.error)


async def search_web(
    query: str,
    time_range: Literal["day", "week", "month", "year"] = "week",
    max_results: int = 5,
) -> list[SearchResult]:
    """æœç´¢ç½‘é¡µï¼ˆå…¼å®¹å‡½æ•°ï¼‰

    æ³¨æ„ï¼šæ­¤å‡½æ•°ä¸ºå…¼å®¹æ¥å£ï¼Œç›´æ¥è¿”å›æ•°æ®è€Œé ToolResultã€‚
    æ–°ä»£ç å»ºè®®ä½¿ç”¨ web_search_tool.execute() è·å–å¸¦é”™è¯¯å¤„ç†çš„ç»“æœã€‚
    """
    result = await web_search_tool.execute(query, time_range, max_results)
    if result.success:
        return result.data
    raise RuntimeError(result.error)

def is_search_engine_available() -> bool:
    """æ£€æŸ¥æœç´¢å¼•æ“æ˜¯å¦å¯ç”¨"""
    return get_search_client() is not None