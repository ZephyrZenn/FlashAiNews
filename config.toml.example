# FlashAiNews Configuration Example
# Copy this file to config.toml and update with your settings

[model]
# Model name to use for generating briefs
model = "gpt-4"

# Provider options: "openai", "deepseek", "gemini", "other"
# API keys are read from environment variables:
#   - openai: OPENAI_API_KEY
#   - deepseek: DEEPSEEK_API_KEY  
#   - gemini: GEMINI_API_KEY
#   - other: MODEL_API_KEY
provider = "openai"

# Base URL is automatically determined for built-in providers:
#   - openai: https://api.openai.com/v1
#   - deepseek: https://api.deepseek.com
#   - gemini: (uses Google SDK)
# 
# Only required when provider = "other":
# base_url = "https://your-custom-api.com/v1"


[rate_limit]
# Rate limiting settings (all fields are optional, defaults shown below)

# Maximum requests per minute to the LLM API
requests_per_minute = 60.0

# Maximum burst size for the token bucket rate limiter
burst_size = 10

# Enable/disable rate limiting (default: true)
enable_rate_limit = true

# Retry settings for handling transient API errors

# Maximum number of retry attempts on failure
max_retries = 3

# Base delay in seconds for exponential backoff
base_delay = 1.0

# Maximum delay in seconds between retries
max_delay = 60.0

# Enable/disable automatic retry on errors (default: true)
enable_retry = true

