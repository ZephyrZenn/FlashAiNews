import logging
import asyncio
import httpx
import trafilatura

logger = logging.getLogger(__name__)


async def get_content(url: str, client: httpx.AsyncClient) -> tuple[str, str | None]:
    """ä½¿ç”¨ httpx + trafilatura å®ç°çš„è¶…è½»é‡æŠ“å–."""
    try:
        # 1. å¼‚æ­¥ä¸‹è½½ç½‘é¡µå†…å®¹
        resp = await client.get(url, timeout=10.0, follow_redirects=True)
        resp.raise_for_status()

        # 2. trafilatura æå–æ­£æ–‡å¹¶ç›´æ¥è½¬ä¸º Markdown
        # include_links=True å¯ä»¥ä¿ç•™é“¾æ¥ï¼Œæ–¹ä¾¿ LLM æº¯æº
        content = trafilatura.extract(
            resp.text, include_links=True, output_format="markdown"
        )

        if content is None:
            error_msg = f"[CRAWLER] âš ï¸ å†…å®¹æå–å¤±è´¥ (trafilaturaè¿”å›ç©º): {url}"
            logger.warning(error_msg)
            print(error_msg)
            return url, None

        return url, content

    except httpx.TimeoutException:
        error_msg = f"[CRAWLER] â±ï¸ è¯·æ±‚è¶…æ—¶: {url}"
        logger.warning(error_msg)
        print(error_msg)
        return url, None

    except httpx.HTTPStatusError as exc:
        error_msg = f"[CRAWLER] âŒ HTTPé”™è¯¯ {exc.response.status_code}: {url}"
        logger.warning(error_msg)
        print(error_msg)
        return url, None

    except httpx.RequestError as exc:
        error_msg = f"[CRAWLER] ğŸ”Œ ç½‘ç»œè¯·æ±‚å¤±è´¥ ({type(exc).__name__}): {url}"
        logger.warning(error_msg)
        print(error_msg)
        return url, None

    except Exception as exc:
        error_msg = f"[CRAWLER] ğŸ’¥ æœªçŸ¥é”™è¯¯ ({type(exc).__name__}: {exc}): {url}"
        logger.error(error_msg)
        print(error_msg)
        return url, None

async def fetch_all_contents(urls: list[str]) -> dict[str, str]:
    """ä½¿ç”¨å¼‚æ­¥ IO æ‰¹é‡æŠ“å–."""
    if not urls:
        return {}

    # ä½¿ç”¨å¼‚æ­¥ Client å…±äº«è¿æ¥æ± 
    async with httpx.AsyncClient(limits=httpx.Limits(max_connections=20)) as client:
        tasks = [get_content(url, client) for url in urls]
        results_list = await asyncio.gather(*tasks)
        return {url: content for url, content in results_list if content}